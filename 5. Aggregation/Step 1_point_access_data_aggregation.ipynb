{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cddb5e8a-fae1-46fa-bd7e-0853dfb1b355",
   "metadata": {},
   "source": [
    "# Administrative Descriptive Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a460cd-35cb-4282-85b3-c4d0ed47e514",
   "metadata": {},
   "source": [
    "After appending travel time information to each populated pixel withon our AOI, we can prepare a number of descriptive stats within administrative units. Given the quantity of data in question these are best prepared with Dask Dataframes. Due to the large number of services that can be investigated and the large number of pixels involved, we export each season-mode-service combination separately, which we will merge together in the next and final notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92997b8-bc23-4481-9872-5ed6a68c8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "import dask\n",
    "import coiled\n",
    "from dask.distributed import Client, LocalCluster, Lock\n",
    "from dask.utils import SerializableLock\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import spatialpandas as sp\n",
    "import dask_geopandas as dg\n",
    "\n",
    "import rioxarray as rx\n",
    "import xarray as xr\n",
    "\n",
    "import re\n",
    "\n",
    "from dask_control import *\n",
    "\n",
    "from spatialpandas.geometry import (\n",
    "    PointArray, MultiPointArray, LineArray,\n",
    "    MultiLineArray, PolygonArray, MultiPolygonArray\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb701ab-bdd8-4ef1-ad39-8c2da5bb1a59",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882c1c6-3555-4ab5-97e6-9d25e9bc97a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_root = 'D:\\\\github_test\\\\'\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "#read project input parameters that will eventually be passed from the UI\n",
    "data_file = data_root + 'project_data.json'\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "#read project variables that will come from UI so that we have our parameters and file locations\n",
    "with open(data_file, 'rb') as f:\n",
    "    data_loaded = json.load(f)\n",
    "f.close()\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "#read information from the project setup file that's relevant to this section of code\n",
    "#imports\n",
    "local_population_folder = data_loaded['local_population_folder']\n",
    "local_lc_folder = data_loaded['local_lc_folder']\n",
    "access_dir = data_loaded['access_dir']\n",
    "dest_crs = data_loaded['dest_crs']\n",
    "dest_crs_id = data_loaded['dest_crs_id']\n",
    "\n",
    "local_boundaries_folder = data_loaded['local_boundaries_folder']\n",
    "level = data_loaded['level']\n",
    "max_level = data_loaded['max_level']\n",
    "if level != 'custom':\n",
    "    shapefile_adm_field = data_loaded['shapefile_adm_field']\n",
    "    adm_name = data_loaded['adm_name']\n",
    "\n",
    "seasons = sorted([os.path.join(local_lc_folder,file) \\\n",
    "            for file \\\n",
    "            in os.listdir(local_lc_folder) \\\n",
    "            if file.endswith(\".tif\")])\n",
    "\n",
    "for strnum in range(0, len(seasons)):\n",
    "    seasons[strnum] = str.replace(seasons[strnum], local_lc_folder,\"\")\n",
    "    seasons[strnum] = str.replace(seasons[strnum], \".tif\",\"\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4294e2-b374-42e4-ada2-8281b4d8fb49",
   "metadata": {},
   "source": [
    "**Initiate Dask Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd58262-d4b0-4e39-8303-7ed6a70053b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client=get_dask_client(cluster_type='local',n_workers=4,processes=True,threads_per_worker=8)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3bcc0-0bc9-4428-9327-ae031cf0caf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3464d-0430-4219-8513-9a41f9a123fe",
   "metadata": {},
   "source": [
    "Pixel level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b26688-3c59-4c11-9dff-9c216cce0401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode_list =['walk','multi']\n",
    "ssn_mode_list = []\n",
    "for ssn in seasons:\n",
    "    for mode_num in range(0,len(mode_list)):\n",
    "        ssn_mode_list.append(ssn + \"_\" + mode_list[mode_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67750ae9-37ec-482f-a3f3-6d1f49f31415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def long_per_indicator(df,indicator,adm_col):\n",
    "      \n",
    "    indic_label = indicator\n",
    "\n",
    "    # pivot the data for just that indicator, with the column VALUES = the population value for that pixel\n",
    "    pop_total = df.pivot_table(index = adm_col, columns=indicator, \n",
    "                 values = 'POP', aggfunc = 'sum', fill_value = 0)\n",
    "\n",
    "    # divide by the rowsum to get the % of population falling in each travel category, per admin area\n",
    "    pop_pct = pop_total.div(np.nansum(pop_total,axis=1),axis=0)\n",
    "    \n",
    "    # create labels\n",
    "    pop_total['indicator'] = indic_label\n",
    "    pop_pct['indicator'] = indic_label\n",
    "    \n",
    "    # remove the multi-index, compress in long format with the adm and indicator as labels, and then change labels/sort\n",
    "    pop_pct = pop_pct.reset_index()\\\n",
    "                            .melt(id_vars=[adm_col,'indicator'])\\\n",
    "                            .rename({indicator:'travel_time_range','value':'pop_pct'},axis=1)\n",
    "    \n",
    "    pop_total = pop_total.reset_index()\\\n",
    "                            .melt(id_vars=[adm_col,'indicator'])\\\n",
    "                            .rename({indicator:'travel_time_range','value':'pop_total'},axis=1)\n",
    "    \n",
    "    long_indic = pd.concat([pop_pct,pop_total[['pop_total']]],axis=1,ignore_index=False)\n",
    "    \n",
    "    return long_indic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531dd76-6e96-4f90-ac72-cc16aa442e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate through all the season-mode combinations and aggregate each service's\n",
    "# pop-weighted access times within each adm unit\n",
    "\n",
    "for ssn_mode_num in range(0, len(ssn_mode_list)):\n",
    "    ssn_mode = ssn_mode_list[ssn_mode_num]\n",
    "    print(ssn_mode)\n",
    "    POINTS_URL = access_dir + 'pixels' + '\\\\' + f\"{ssn_mode}_df_pixels_final-*.csv\"\n",
    "\n",
    "    df_pixels_source = dd.read_csv(POINTS_URL,header=0,\n",
    "                                            na_values = ' ',\n",
    "                                            blocksize=100e6)\n",
    "\n",
    "    # Get the list of column names\n",
    "    column_names = df_pixels_source.columns\n",
    "\n",
    "    # List to store the column indices\n",
    "    column_indices = []\n",
    "\n",
    "    # Loop through column names to find the ones ending with \"_avg_adm2\" or \"_avg_adm3\"\n",
    "    for idx, col_name in enumerate(column_names):\n",
    "        if col_name.endswith('_avg_adm2') or col_name.endswith('_avg_adm3'):\n",
    "            column_indices.append(idx)\n",
    "                 \n",
    "    min_col_num = column_indices[0]+1\n",
    "    max_col_num = column_indices[len(column_indices)-1]+2\n",
    "\n",
    "    # read each service's pop-weighted access times within adm units at a specific adm level, and aggregate\n",
    "    for col_num in range(min_col_num, max_col_num):\n",
    "\n",
    "        if max_level == 'adm2':\n",
    "            df_pixels = dd.multi.concat([df_pixels_source[[\"ADM2_EN\",\"Adm2_Code\",\"wt_adm_2\"]],df_pixels_source.iloc[: , [col_num-1]]],axis=1)\n",
    "        else:\n",
    "            df_pixels = dd.multi.concat([df_pixels_source[[\"ADM3_EN\",\"ADM2_EN\",\"Adm3_Code\",\"Adm2_Code\",\"wt_adm_3\",\"wt_adm_2\"]],df_pixels_source.iloc[: , [col_num-1]]],axis=1)\n",
    "        \n",
    "        df_pixels = df_pixels.persist()\n",
    "\n",
    "        float64_cols = list(df_pixels.select_dtypes(include='float64'))\n",
    "\n",
    "        # The same code again calling the columns\n",
    "        df_pixels[float64_cols] = df_pixels[float64_cols].astype('float32')\n",
    "\n",
    "        if max_level == 'adm2':\n",
    "            access_col = df_pixels.columns[3]\n",
    "        else:\n",
    "            access_col = df_pixels.columns[6]\n",
    "\n",
    "        adm_level = access_col[-1]\n",
    "\n",
    "        if adm_level == '2':\n",
    "            adm_code = 'Adm2_Code'\n",
    "            adm_frame = df_pixels[['ADM2_EN', 'Adm2_Code']].copy()\n",
    "            adm_frame = adm_frame.drop_duplicates(subset='ADM2_EN')\n",
    "            adm_frame = adm_frame.sort_values(\"ADM2_EN\")\n",
    "        else:\n",
    "            adm_code = 'Adm3_Code'    \n",
    "            adm_frame = df_pixels.copy()\n",
    "            adm_frame = df_pixels[['ADM3_EN','ADM2_EN','Adm3_Code','Adm2_Code']].copy()\n",
    "            adm_frame = adm_frame.drop_duplicates(subset='ADM3_EN')\n",
    "            adm_frame = adm_frame.sort_values(\"ADM3_EN\")\n",
    "\n",
    "        #######################################################################\n",
    "        #AGGREGATION\n",
    "        \n",
    "        adm_aggr = df_pixels.groupby([adm_code])[access_col].sum().reset_index()\n",
    "\n",
    "        #INDEXING\n",
    "        \n",
    "        min_acc = np.array(np.min(adm_aggr[access_col],axis=0)) # column-wise min\n",
    "        max_acc = np.array(np.max(adm_aggr[access_col],axis=0)) # column-wise max\n",
    "        \n",
    "        adm_aggr[access_col+'_idx'] = (max_acc - adm_aggr[access_col]) / (max_acc - min_acc)\n",
    "\n",
    "        #Finalise\n",
    "        \n",
    "        adm_aggr_final = adm_frame.copy()\n",
    "        adm_aggr_final = adm_aggr_final.merge(adm_aggr,on=adm_code)\n",
    "\n",
    "        adm_aggr_final.to_csv(access_dir + 'tables' + '\\\\separate\\\\' + f\"{access_col}_final.csv\", single_file=True, index=False)\n",
    "\n",
    "        #######################################################################\n",
    "        #BINNING\n",
    "        # here we determine some more in-depth stats, that determine the % adm population within travel time bins to the specific service for the ssn-mode in question\n",
    "\n",
    "        access_col_trim = access_col[:-9]\n",
    "        \n",
    "        if max_level == 'adm2':\n",
    "            dft = dd.multi.concat([df_pixels_source[[\"POP\",adm_code]],df_pixels_source[access_col_trim]],axis=1)\n",
    "        else:\n",
    "            dft = dd.multi.concat([df_pixels_source[[\"POP\",adm_code]],df_pixels_source[access_col_trim]],axis=1)\n",
    "\n",
    "        dft = dft.compute()\n",
    "\n",
    "        # Travel time ranges\n",
    "        tt_bins = [0, 0.5, 1, 2, 4, 8, 16, 10000]\n",
    "        tt_bin_labels = [\"0 - 30 minutes\", \"31 - 60 minutes\", \"1 - 2 hours\", \"2 - 4 hours\", \"4 - 8 hours\", \"8 - 16 hours\", \"16+ hours\"]\n",
    "\n",
    "        # rename dict\n",
    "        tt_rename_dct = {\n",
    "            1 : \"0 - 30 minutes\",\n",
    "            2 : \"31 - 60 minutes\",\n",
    "            3 : \"1 - 2 hours\",\n",
    "            4 : \"2 - 4 hours\",\n",
    "            5 : \"4 - 8 hours\",\n",
    "            6 : \"8 - 16 hours\",\n",
    "            7 : \"16+ hours\"}\n",
    "\n",
    "        dft[access_col_trim] = pd.DataFrame(np.digitize(dft[access_col_trim], bins=tt_bins),columns=[access_col_trim], index=dft.index)\n",
    "        dft[access_col_trim] = dft[access_col_trim].replace(tt_rename_dct)\n",
    "\n",
    "        # For each indicator, pivot data by administrative unit, calculate the pct of total population per travel time bin, and reshape the data into a long format.</br>Then merge all these reshaped long tables into one master table \n",
    "        long_data_lst_adm = []\n",
    "        \n",
    "        long_i_adm = long_per_indicator(dft,access_col_trim,adm_code) \n",
    "        long_data_lst_adm.append(long_i_adm)\n",
    "\n",
    "        # concatenate\n",
    "        long_acc_indicators_adm = pd.concat(long_data_lst_adm,ignore_index=True)\n",
    "\n",
    "        # convert tt ranges to categorical and order appropriately\n",
    "        long_acc_indicators_adm['travel_time_range'] = long_acc_indicators_adm.travel_time_range.astype('category').cat.set_categories(tt_bin_labels)\n",
    "\n",
    "        # # order as desired\n",
    "        long_acc_indicators_adm = long_acc_indicators_adm.sort_values([adm_code,'indicator','travel_time_range']).reset_index(drop=True)\n",
    "\n",
    "        # Calculate cumulative sums per indicator and the Adm2_Code for Adm3 datasets\n",
    "        long_acc_indicators_adm['pop_pct_csum'] = long_acc_indicators_adm.groupby([adm_code,'indicator'])['pop_pct'].cumsum(axis=0)\n",
    "        long_acc_indicators_adm['pop_total_csum'] = long_acc_indicators_adm.groupby([adm_code,'indicator'])['pop_total'].cumsum(axis=0)\n",
    "\n",
    "        float64_cols = list(long_acc_indicators_adm.select_dtypes(include='float64'))\n",
    "        long_acc_indicators_adm[float64_cols] = long_acc_indicators_adm[float64_cols].astype('float32')\n",
    "        \n",
    "        # Export final long data\n",
    "        adm_long_final = adm_frame.copy()\n",
    "        adm_long_final = adm_long_final.merge(long_acc_indicators_adm,on=adm_code)\n",
    "        \n",
    "        adm_long_final.to_csv(access_dir + 'tables' + '\\\\separate\\\\' + f\"{access_col}_acc_indicators_long.csv\", single_file=True, index = False)\n",
    "\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (git)",
   "language": "python",
   "name": "git"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
